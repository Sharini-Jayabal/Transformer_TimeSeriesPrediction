import torch
import torch.nn as nn
import numpy as np
import scipy.io
import matplotlib.pyplot as plt

class Transformer(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers, num_heads, sequence_length):
        super().__init__()
        self.sequence_length = sequence_length
        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(input_size, num_heads, hidden_size), num_layers)
        self.decoder = nn.Linear(hidden_size, output_size)
        self.output_size = output_size 
        self.input_size = input_size
        self.hidden_size = hidden_size
        
    def forward(self, x):
        x = x.permute(1, 0, 2)
        x = self.encoder(x)
        x = x[-1]
        self.decoder = nn.Linear(hidden_size, output_size)
        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size, sparse=True)
        return x

data = scipy.io.loadmat('/content/simulation1_data.mat')
gripper_pos = data['gripperPosition_history']

# Define the co-ord position (Range: 0 - 2)
coord = gripper_pos[:, 1, :]                 #<-- Change gripper_pos[:, 0, :] (or) Change gripper_pos[:, 1, :] (or) Change gripper_pos[:, 2, :]

for i in range(gripper_pos.shape[2]):
    plt.plot(coord[:, i])

plt.xlabel('Time step')
plt.ylabel(f'Gripper position (coordinate)')
plt.show()

sequence_length = 30
train_data = coord[:300, :]
test_data = coord[300:, :]

def prepare_data(data):
    X, y = [], []
    for i in range(sequence_length, len(data)):
        X.append(data[i-sequence_length:i, :])
        y.append(data[i, :])
    return np.array(X), np.array(y)


X_train, y_train = prepare_data(train_data)
X_test, y_test = prepare_data(test_data)

# Convert the data to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)
X_test = X_test.unsqueeze(1) 

# Defining hyperparameters
input_size = gripper_pos.shape[2] 
hidden_size = 32
output_size = 30
num_layers = 5
num_heads = 3
batch_size = 32
num_epochs = 50
learning_rate = 0.01

assert input_size % num_heads == 0, "input_size must be divisible by num_heads"

model = Transformer(input_size, hidden_size, output_size, num_layers, num_heads, sequence_length)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
train_losses = []
test_losses = []
predictions = []
ground_truths = []

for epoch in range(num_epochs):
  for i in range(0, len(X_train), batch_size):
    x_batch = X_train[i:i+batch_size]
    y_batch = y_train[i:i+batch_size]
    y_pred = model(x_batch)
    loss = criterion(y_pred[:, 0], y_batch[:, 0])
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    train_losses.append(loss.item())
    predictions.append(y_pred.detach().numpy())
    ground_truths.append(y_batch.detach().numpy())

with torch.no_grad():
    test_loss = 0
    for i in range(0, len(X_test), batch_size):
        x_batch = X_test[i:i+batch_size]
        y_batch = y_batch.unsqueeze(1) 
        y_pred = model(x_batch)
        loss = criterion(y_pred, y_batch)
        
        test_loss += loss.item()
        predictions.append(y_pred.detach().numpy())
        ground_truths.append(y_batch.detach().numpy())
        
    test_losses.append(test_loss)  
    
print(f"Epoch {epoch+1}, train loss: {train_losses[-1]:.4f}, test loss: {test_losses[-1]:.4f}")

# Convert predictions and ground truths to numpy arrays
predictions = np.concatenate(predictions)
ground_truths = np.concatenate(ground_truths)

# Plot the ground truth and predicted values
fig, ax = plt.subplots(figsize=(10, 5))
ax.plot(predictions[:60, 0], label='Predicted', color='cyan')
ax.plot(ground_truths[:30, 0], label='Ground Truth', color='green')
ax.set_xlabel('Time step')
ax.set_ylabel(f'Gripper position (coordinate)')
ax.legend()
plt.show()
